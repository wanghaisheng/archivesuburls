name: Wayback Scraper

on:
  workflow_dispatch:
    inputs:
      url:
        description: 'The URL or domain to scrape (e.g., www.google.com)'
        required: true
        default: 'www.google.com'  # Default value for the URL
      from_date:
        description: 'The starting year for the Wayback Machine (e.g., 2020)'
        required: true
        default: '2020'  # Default start date
      to_date:
        description: 'The ending year for the Wayback Machine (e.g., 2023)'
        required: true
        default: '2023'  # Default end date

jobs:
  scraper:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v2

    - name: Set up Python 3.x
      uses: actions/setup-python@v2
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests

    - name: Run Wayback Scraper
      run: |
        python main.py ${{ github.event.inputs.url }} ${{ github.event.inputs.from_date }} ${{ github.event.inputs.to_date }}
      
    - name: Commit and push CSV file to repo
      run: |
        # Configure git user for commit
        git config --global user.name "GitHub Actions"
        git config --global user.email "actions@github.com"
        
        # Add the generated CSV file to Git
        git add wayback_links.csv
        
        # Commit the change
        git commit -m "Add scraped Wayback Machine links CSV"
      
        # Push the commit to the repository
        git push origin HEAD
